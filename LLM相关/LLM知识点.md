# LLM相关知识点

### 分词 tokenizen

分词方法

- 转化为单个字
- 切分词语

N-gram：准备词语特征的方法

1 2 3 4 5

### 向量化

1.one-hot编码

2.word embedding

- 稠密矩阵
- m行n列：m个数，n维度

假设 5万个词

one-hot：[5万，5万]

word embedding [5万，维数（开始是超参数，经过训练可确定）]

词——>数字——>向量（中间涉及形状的变化）

word embedding API

torch.nn.Embedding(词典大小，嵌入维度)

```python
embed = torch.nn.Embedding(vocab_size,300)#实例化
input_embed = embed(input)#操作
```

### tokenizer.model

在语言模型中，tokenizer.model文件通常是指预训练模型中的词表和词嵌入（word embeddings）等信息。这个文件包含了模型在训练过程中学到的单词（或子词）的表示形式，通常以一种特定的格式保存。

- 1.词表（Vocabulary）: tokenizer.model文件中通常包含了模型所使用的词汇表或者子词表。这个词表中包含了模型在训练过程中遇到的单词或者子词，并为每个单词或者子词分配了一个唯一的标识符（通常是整数）。
- 2.词嵌入（Word Embeddings）: 这个文件可能也包含了每个单词或者子词的词嵌入向量。词嵌入向量是将单词或者子词映射到一个连续的向量空间中，以便模型可以更好地理解和处理文本数据。这些向量表示了单词或者子词在语义上的相似性和关联性。
- 3.特殊标记（Special Tokens）: tokenizer.model文件还可能包含一些特殊标记，如句子的开始（）、结束（）标记等。这些标记在模型的输入中用于指示文本的开始或结束，或者用于表示特定的语义信息。

这个文件通常由预训练模型的tokenizer生成并使用，在使用预训练模型进行文本处理（如编码文本、生成词嵌入等）时，这个文件提供了模型需要的词汇信息和相关的嵌入表示，使得模型能够正确地理解和处理文本数据。

### 形状变化

**思考**:每个batch中的每个句子有10个词语，经过形状为【20 ， 4】的词嵌入后句子会变成什么形状?
	即每个词语用长度为4的向量表示，所以，最终句子会变为[batch_size,10,4]的形状增加了一个维度，这个维度是embedding的dim.

[batch_size, seq_len]——> [batch_size, seq_len,dim]

## 掩码

**因果掩码**

**causal mask** : 主要用于限定模型的可视范围，防止模型看到未来的数据

例子：GPT codellama phi-2

自注意力（self-attention）是在整个令牌（token）序列上计算的，包括当前令牌之后的令牌。

将所有未来的令牌设置为零，有效地从注意力机制中屏蔽了它们

具体实现中，这种掩码可以通过原始输入和一个合适的上三角矩阵相乘（或者逻辑与）来得到

```
# Causal mask
causal_mask = torch.triu(torch.ones(input_shape[1], input_shape[1]), diagonal=1).bool().to(input.device)
```

1. 训练式位置编码

- 将位置编码当作可训练参数，训练一个位置编码向量矩阵。. GPT3就采用了这种方式。

- 缺点是没有外推性。

2.旋转位置编码RoPE

- 作用在每个transformer层的self-attention块，在计算完Q/K之后，旋转位置编码作用在Q/K上，再计算attention score。

- 旋转位置编码通过绝对编码的方式实现了相对位置编码，有良好的外推性。. LLaMA、GLM-130B、PaLM等大语言模型就采用了旋转位置编码RoPE。

3.ALiBi (Attention with Linear Biases)

- 在计算完attention score后，直接为attention score矩阵加上一个预设好的偏置矩阵。- ALiBi位置编码有良好的外推性。

- BLOOM就采用了这种位置编码。

## Cache

在大型语言模型中，"cache" 缓存通常指的是**在生成文本序列时保存的中间状态信息**。这个缓存的**目的是为了在生成下一个词时重复使用之前计算的信息，从而提高生成效率**。
在基于注意力机制的模型中，特别是像Transformer这样的架构，生成每个词时都需要考虑输入序列中所有位置的信息。为了避免重复计算，模型会将先前计算的注意力权重和编码器输出等信息存储在缓存中。这些缓存的内容通常是一个张量（tensor）或一组张量，包含了先前层次的计算结果。
具体而言，对于一个解码器来说，cache 通常包括：

1.先前计算的注意力权重： 这些权重表示了在生成当前词时模型关注的输入序列中的哪些位置。通过缓存这些权重，模型可以在生成下一个词时重复使用这些信息，而不必重新计算。
2.编码器输出的缓存： 缓存中可能包括编码器输出的某些部分，以便在生成下一个词时直接使用，而无需重新运行整个编码器。

通过使用缓存，模型可以更高效地处理长文本序列，避免在每个时间步骤都重新计算相同的信息，从而提高了生成速度。这对于处理长文本或进行实时推理任务是非常重要的。

## MOE

**混合专家模型**，英文叫Mixture of Experts (MoE) 是一种模型设计策略。它通过**将多个模型（称为"专家"）直接结合在一起**，以获得更好的预测性能。

在大模型中，MoE方案可以有效地提高模型的容量和效率。一般而言，大模型的MoE有**一个门控机制和一套门控输出机制**来**合并和平衡专家的选择**，用于决定每个专家对最终预测的；有一套专家选择机制，会根据门控机制的输出选择一部分专家进行预测。这样可以减少计算量，并使模型能够针对不同的输入选择最合适的专家。还有一套训练机制。
